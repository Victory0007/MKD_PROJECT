{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb6c0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('search_urls.txt', 'w+')\n",
    "url = 'https://www.amazon.com/s?k=lunch+bag+pink+poladot&crid=HVWM90RPPDMQ&sprefix=lunch+bag+pink+polado%2Caps%2C366&ref=nb_sb_noss_1'\n",
    "f.write(url)\n",
    "f.write('\\n')\n",
    "\n",
    "\n",
    "for i in range(2,51):\n",
    "    url = \"https://www.amazon.com/s?k=LUNCH+BAG+PINK+POLKADOT&page=\" + str(i) +\"&crid=3CPJO8LCRN16G&qid=1718276686&sprefix=lunch+bag+pink+polkadot%2Caps%2C470&ref=sr_pg_\" + str(i)\n",
    "    f.write(url)\n",
    "    f.write('\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selectorlib\n",
    "from selectorlib import Extractor\n",
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "\n",
    "# Create an Extractor by reading from the YAML file\n",
    "e = Extractor.from_yaml_file('search.yml')\n",
    "\n",
    "\n",
    "def scrape(url):\n",
    "    ua = UserAgent()\n",
    "\n",
    "    headers = {\n",
    "        'dnt': '1',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': ua.random,\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-user': '?1',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'referer': 'https://www.amazon.com/',\n",
    "        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "    }\n",
    "\n",
    "    # Download the page using requests\n",
    "    print(\"Downloading %s\" % url)\n",
    "    r = requests.get(url, headers=headers)\n",
    "    # Simple check to check if page was blocked (Usually 503)\n",
    "    if r.status_code > 500:\n",
    "        if \"To discuss automated access to Amazon data please contact\" in r.text:\n",
    "            print(\n",
    "                \"Page %s was blocked by Amazon. Please try using better proxies\\n\" % url)\n",
    "        else:\n",
    "            print(\"Page %s must have been blocked by Amazon as the status code was %d\" % (\n",
    "                url, r.status_code))\n",
    "        return None\n",
    "    # Pass the HTML of the page and create\n",
    "    return e.extract(r.text)\n",
    "\n",
    "\n",
    "# product_data = []\n",
    "with open(\"search_urls.txt\", 'r') as urllist, open('search_output.jsonl', 'w') as outfile:\n",
    "    f = open('product_urls.txt','w')\n",
    "    for url in urllist.read().splitlines():\n",
    "        data = scrape(url)\n",
    "\n",
    "       \n",
    "        if data:\n",
    "            try:\n",
    "                for product in data['products']:\n",
    "                    product['url'] = 'https://www.amazon.com' + product['url']\n",
    "                    f.write(product['url'])\n",
    "                    f.write('\\n')\n",
    "\n",
    "                    json.dump(product, outfile)\n",
    "                    outfile.write(\", \\n\")\n",
    "                    # sleep(5)\n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
